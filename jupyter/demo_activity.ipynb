{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "from ast import literal_eval\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import geohash\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import mean, min, max, sum\n",
    "from pyspark.sql import functions as func\n",
    "import glob\n",
    "import matplotlib.colors\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# folder path\n",
    "resultsdir= \"\"\n",
    "\n",
    "# attributes\n",
    "fn=\"_all_counts.csv\"\n",
    "fn_age=\"_age.csv\"\n",
    "fn_gender=\"_gender.csv\"\n",
    "\n",
    "# load targetzones(NRW)\n",
    "tz=pd.read_csv(resultsdir + \"170830_munich_NW_MTC_unambiguous.csv\")\n",
    "tz_list=map(int,list(tz[\"id\"].values))\n",
    "\n",
    "# load mcc list\n",
    "mccs=pd.read_csv(\"MCCs.csv\")\n",
    "\n",
    "# Time mapping\n",
    "time_mapping={2:\"2\",3:\"3\",4:\"4\",5:\"5\",6:\"6\",7:\"7\",8:\"8\",9:\"9\",10:\"10\",11:\"11\",12:\"12\",13:\"13\",14:\"14\",15:\"15\",\\\n",
    "             16:\"16\",17:\"17\",18:\"18\",19:\"19\",20:\"20\",21:\"21\",22:\"22\",23:\"23\",0:\"0+1\",1:\"1+1\"}\n",
    "hh=[\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\\\n",
    "             \"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"0+1\",\"1+1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cluster to analyze\n",
    "weekday_cluster = \"FRI\"\n",
    "\n",
    "# This is the number of days that were aggregated\n",
    "number_of_days=4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151255817.118\n",
      "Anzahl groups: 174\n",
      "Anzahl Kombinationen: 4176\n",
      "13750529\n"
     ]
    }
   ],
   "source": [
    "#save for customer\n",
    "\n",
    "nf=pd.read_csv(weekday_cluster+fn)\n",
    "nf=nf[nf[\"dominant_zone\"].isin(tz_list)]\n",
    "nf=nf.sort_values(\"count\",ascending=0)\n",
    "nf=nf[nf[\"count\"]>0]\n",
    "sum_counts=np.sum(nf[\"count\"].values)\n",
    "\n",
    "print(sum_counts)\n",
    "\n",
    "\n",
    "nf[\"hour\"]=nf.apply(lambda row:(time_mapping[int(row[\"time\"].split(':')[0])]),axis=1)\n",
    "nf[\"count\"]=nf.apply(lambda row:int(np.round(row[\"count\"]/number_of_days,0)),axis=1)\n",
    "nf=nf.rename(columns={\"dominant_zone\":\"zone\"})\n",
    "nf=nf.drop([\"time\"],1)\n",
    "nf=nf.sort_values(\"count\",ascending=0)\n",
    "sum_counts=np.sum(nf[\"count\"].values)\n",
    "print(\"Anzahl groups: \" + str(len(np.unique(nf[\"zone\"].values))))\n",
    "print(\"Anzahl Kombinationen: \" + str(len(nf)))\n",
    "print(sum_counts)\n",
    "nf[\"cluster\"] = weekday_cluster\n",
    "#save as a flat table\n",
    "nf.to_csv(resultsdir+\"Munich_NW_\"+weekday_cluster+\"_all_counts.csv\",index=False)\n",
    "\n",
    "#save as matrix\n",
    "nf=nf.reset_index()\n",
    "res=pd.DataFrame(index=list(np.unique(nf[\"zone\"].values)),columns=hh)\n",
    "for l in range(0,len(nf)):\n",
    "    res.loc[nf[\"zone\"][l],nf[\"hour\"][l]]=nf[\"count\"][l]\n",
    "res=res.fillna(0)\n",
    "res.to_csv(resultsdir+\"Munich_NW_\"+weekday_cluster+\"_all_counts_matrix.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11542848\n",
      "11541661\n"
     ]
    }
   ],
   "source": [
    "#age \n",
    "\n",
    "nf=pd.read_csv(resultsdir+weekday_cluster+fn_age)\n",
    "nf=nf[nf[\"dominant_zone\"].isin(tz_list)]\n",
    "print(sum_counts)\n",
    "\n",
    "nf[\"hour\"]=nf.apply(lambda row:(time_mapping[int(row[\"time\"].split(':')[0])]),axis=1)\n",
    "nf[\"count\"]=nf.apply(lambda row:int(np.round(row[\"count\"]/number_of_days,0)),axis=1)\n",
    "nf=nf.rename(columns={\"dominant_zone\":\"zone\"})\n",
    "nf=nf[nf[\"count\"]>0]\n",
    "sum_counts=np.sum(nf[\"count\"].values)\n",
    "print(sum_counts)\n",
    "\n",
    "nf[\"cluster\"] = weekday_cluster\n",
    "nf=nf[nf[\"age\"].isin([1,2,3,4,5,6])]\n",
    "nf=nf.reset_index()\n",
    "nf[\"sum_age_info\"]=nf.groupby([\"zone\",\"hour\"])[\"count\"].transform(np.sum)\n",
    "nf[\"share\"]=nf.apply(lambda row:row[\"count\"]/float(row.sum_age_info),axis=1)\n",
    "\n",
    "nf=nf.drop([\"time\",\"sum_age_info\",\"count\",\"index\"],1)\n",
    "nf.to_csv(resultsdir+\"Munich_NW_\"+weekday_cluster+\"_age.csv\",index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11545768\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nf=pd.read_csv(resultsdir+weekday_cluster+fn_gender)\n",
    "nf=nf[nf[\"dominant_zone\"].isin(tz_list)]\n",
    "nf[\"hour\"]=nf.apply(lambda row:(time_mapping[int(row[\"time\"].split(':')[0])]),axis=1)\n",
    "nf[\"count\"]=nf.apply(lambda row:int(np.round(row[\"count\"]/number_of_days,0)),axis=1)\n",
    "nf=nf.rename(columns={\"dominant_zone\":\"zone\"})\n",
    "nf=nf[nf[\"count\"]>0]\n",
    "sum_counts=np.sum(nf[\"count\"].values)\n",
    "print(sum_counts)\n",
    "\n",
    "nf[\"cluster\"] = weekday_cluster\n",
    "nf=nf[nf[\"gender\"].isin([1,2])]\n",
    "nf=nf.reset_index()\n",
    "nf[\"sum_gender_info\"]=nf.groupby([\"zone\",\"hour\"])[\"count\"].transform(np.sum)\n",
    "nf[\"share\"]=nf.apply(lambda row:row[\"count\"]/float(row.sum_gender_info),axis=1)\n",
    "\n",
    "nf=nf.drop([\"time\",\"sum_gender_info\",\"count\",\"index\"],1)\n",
    "nf.to_csv(resultsdir+\"Munich_NW_\"+weekday_cluster+\"_gender.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
